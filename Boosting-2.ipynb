{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f248bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique that builds an ensemble of weak learners (typically decision trees)\n",
    "sequentially. It fits the new model to the residual errors made by the previous model, gradually reducing the error in \n",
    "predictions. The final prediction is the sum of predictions from all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bc2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3379888778506104\n",
      "R-squared: 0.9990403356176427\n"
     ]
    }
   ],
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an\n",
    "# example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and \n",
    "# R-squared.\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.base_prediction = np.mean(y)\n",
    "        prediction = np.full_like(y, self.base_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - prediction\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.models.append(tree)\n",
    "            prediction += self.learning_rate * tree.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = np.full(len(X), self.base_prediction)\n",
    "        for model in self.models:\n",
    "            prediction += self.learning_rate * model.predict(X)\n",
    "        return prediction\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate some sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the \n",
    "# performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "A weak learner in Gradient Boosting is a simple model that performs slightly better than random chance on a given problem. In the context of decision trees, a weak learner might be a decision tree with shallow depth.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to sequentially fit a sequence of weak learners to the residuals of the previous model. This approach gradually reduces the error in predictions by focusing on the mistakes made by the previous models.\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners by iteratively training new models to correct the errors made by the previous models. Each new model focuses on the residuals (errors) of the previous model, gradually reducing the overall error in predictions.\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm typically include:\n",
    "\n",
    "Initializing the model with a constant value (e.g., the mean of the target variable).\n",
    "Calculating the residuals (errors) between the predictions of the current model and the true values.\n",
    "Training a new model (weak learner) to predict these residuals.\n",
    "Updating the current model by adding the predictions of the new model, scaled by a learning rate.\n",
    "Repeat steps 2-4 until a predefined number of iterations or until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83468cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

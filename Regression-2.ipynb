{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94449ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. R-squared in Linear Regression:\n",
    "\n",
    "R-squared, or the coefficient of determination, measures the proportion of the variance in the dependent variable that is\n",
    "predictable from the independent variable(s).\n",
    "It is calculated as the ratio of the explained variance to the total variance. The formula is \n",
    "SST is the total sum of squares.\n",
    "R-squared ranges from 0 to 1; a higher value indicates a better fit of the model to the data.\n",
    "Q2. Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is a modification of R-squared that accounts for the number of predictors in the model.\n",
    "It penalizes the inclusion of irrelevant predictors that do not contribute significantly to explaining the variance.\n",
    "\n",
    "Q3. Appropriate Use of Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors.\n",
    "It helps prevent overfitting by considering the model's complexity.\n",
    "It is useful for assessing the goodness of fit when the number of predictors varies between models.\n",
    "\n",
    "Q4. Regression Metrics - RMSE, MSE, MAE:\n",
    "\n",
    "Root Mean Squared Error (RMSE): It is the square root of the average of squared differences between predicted and actual\n",
    "values. RMSE provides a measure of the average magnitude of errors.\n",
    "Mean Squared Error (MSE): It is the average of squared differences between predicted and actual values.\n",
    "Mean Absolute Error (MAE): It is the average of absolute differences between predicted and actual values.\n",
    "Interpretation:\n",
    "\n",
    "Lower values of RMSE, MSE, and MAE indicate better model performance.\n",
    "RMSE and MSE give more weight to larger errors, while MAE treats all errors equally.\n",
    "Use Cases:\n",
    "RMSE, MSE, and MAE are commonly used metrics for assessing the accuracy of regression models and comparing different models.\n",
    "\n",
    "Q5. Advantages and Disadvantages of Evaluation Metrics in Regression:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "RMSE/MSE: Sensitive to large errors, useful when large errors should be penalized more.\n",
    "MAE: Robust to outliers, provides a straightforward interpretation of average error.\n",
    "Disadvantages:\n",
    "\n",
    "RMSE/MSE: Sensitive to outliers, squares errors, which can amplify the impact of large errors.\n",
    "MAE: Ignores the magnitude of errors, treating all errors equally.\n",
    "Choosing a Metric:\n",
    "\n",
    "RMSE/MSE: Suitable when the model needs to penalize large errors more (e.g., predicting house prices).\n",
    "MAE: Suitable when the model should be robust to outliers (e.g., predicting income).\n",
    "Q6. Lasso Regularization:\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty term to the linear regression cost function, which is\n",
    "the absolute value of the coefficients multiplied by a regularization parameter\n",
    "It encourages sparsity, making some coefficients exactly zero and effectively selecting features.\n",
    "Lasso can be useful for feature selection and simplifying models.\n",
    "Differences from Ridge Regularization:\n",
    "\n",
    "Ridge adds a penalty term based on the square of the coefficients, while Lasso uses the absolute value.\n",
    "Ridge tends to shrink coefficients towards zero, but rarely makes them exactly zero.\n",
    "Lasso has the ability to eliminate some features entirely.\n",
    "Q7. Preventing Overfitting with Regularized Linear Models:\n",
    "\n",
    "Regularization techniques like Ridge and Lasso penalize large coefficients, preventing the model from becoming too complex.\n",
    "Example: In a Ridge regression model predicting housing prices, without regularization, the model might assign high weights to\n",
    "    all features, including irrelevant ones. Regularization helps shrink these weights, preventing overfitting to the training\n",
    "    data.\n",
    "Q8. Limitations of Regularized Linear Models:\n",
    "\n",
    "Loss of Interpretability: Regularization can make coefficients harder to interpret, especially in Lasso where some may be\n",
    "    exactly zero.\n",
    "Parameter Tuning: Selecting the appropriate regularization strength (\n",
    "Î») can be challenging and may require cross-validation.\n",
    "Not Suitable for All Cases: Regularization might not be necessary if the dataset is small, or the true relationship is simple.\n",
    "    In such cases, traditional linear regression may perform better.\n",
    "    \n",
    "Q9. Choosing Between Models A and B:\n",
    "\n",
    "The choice between RMSE and MAE depends on the specific requirements of the problem.\n",
    "If you prioritize smaller errors being more heavily penalized, choose Model A with RMSE.\n",
    "If you prefer a metric that is robust to outliers and treats all errors equally, choose Model B with MAE.\n",
    "Limitations of the Choice:\n",
    "\n",
    "The choice between RMSE and MAE might not always be clear-cut, and it depends on the characteristics of the data and the \n",
    "problem.\n",
    "It's advisable to consider the nature of errors and the impact of outliers when making the decision.\n",
    "Q10. Choosing Between Ridge (Model A) and Lasso (Model B):\n",
    "\n",
    "The choice between Ridge and Lasso regularization depends on the specific characteristics of the dataset and the goals of the \n",
    "modeling.\n",
    "If feature selection is crucial and you want to encourage sparsity, Lasso might be preferred.\n",
    "If you want to shrink coefficients towards zero without eliminating them entirely, Ridge might be more suitable.\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "Ridge: Shrinkage towards zero is more gradual, and all features are retained. It may not perform well if there are truly\n",
    "    irrelevant features.\n",
    "Lasso: Can eliminate some features entirely, providing a more interpretable model. However, it may be sensitive to correlated\n",
    "    features, and the choice of the regularization parameter is critical.\n",
    "Parameter Tuning: The performance of these regularization methods is sensitive to the choice of the regularization parameter.\n",
    "    It might require cross-validation to find the optimal value.\n",
    "Interpretability: Both Ridge and Lasso can make coefficients less interpretable due to the shrinkage effect.\n",
    "Considerations:\n",
    "\n",
    "Assess the goals of the modeling (feature selection, coefficient shrinkage).\n",
    "Use cross-validation to tune regularization parameters.\n",
    "Understand the trade-offs between interpretability and complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114295a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: The kernel trick in SVM allows the algorithm to implicitly map the input features into a higher-dimensional space without\n",
    "    explicitly computing the transformation. Instead of working directly in the input space, SVMs can operate in a feature\n",
    "    space defined by a kernel function. This allows SVMs to handle non-linear decision boundaries. Common kernel functions\n",
    "    include polynomial kernels and radial basis function (RBF) kernels.\n",
    "\n",
    "Q4: Support vectors in SVM are the data points that lie closest to the decision boundary (hyperplane). They play a crucial\n",
    "    role in defining the decision boundary and are the ones that contribute to the computation of the margin. Support vectors \n",
    "    are the only data points that affect the position and orientation of the hyperplane. All other data points are essentially\n",
    "    ignored.\n",
    "\n",
    "Example:\n",
    "Consider a binary classification problem with two classes (positive and negative). The support vectors are the data points\n",
    "from each class that are closest to the opposing class. They define the margins and the hyperplane. In the figure below, the\n",
    "circles and crosses represent two classes, and the support vectors are highlighted with larger markers.\n",
    "\n",
    "\n",
    "Q5: Illustration of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM:\n",
    "\n",
    "Hyperplane: The hyperplane is the decision boundary that separates different classes. In a two-dimensional space, it is a line,\n",
    "    and in higher dimensions, it becomes a hyperplane.\n",
    "\n",
    "Margin: The margin is the distance between the hyperplane and the nearest data points from each class. It is maximized in SVM\n",
    "    to improve generalization.\n",
    "\n",
    "Soft Margin: In a soft-margin SVM, some data points are allowed to violate the margin or even the hyperplane, introducing a\n",
    "    level of tolerance for outliers or misclassified points.\n",
    "\n",
    "Hard Margin: In a hard-margin SVM, all data points are required to be correctly classified and lie outside the margin. This \n",
    "    makes the model more sensitive to outliers.\n",
    "\n",
    "\n",
    "Q6: SVM Implementation through Iris dataset:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Taking only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "linear_svc = SVC(kernel='linear')\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = linear_svc.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', marker='o')\n",
    "\n",
    "# Plot the decision boundaries\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
    "Z = linear_svc.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# Highlight support vectors\n",
    "plt.scatter(linear_svc.support_vectors_[:, 0], linear_svc.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n",
    "plt.title('Linear SVM Decision Boundaries')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

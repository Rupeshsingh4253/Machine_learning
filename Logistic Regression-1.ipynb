{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Difference between Linear Regression and Logistic Regression:\n",
    "\n",
    "Linear Regression:\n",
    "\n",
    "Used for predicting a continuous outcome variable.\n",
    "The output is the weighted sum of input features plus a bias term.\n",
    "Example: Predicting house prices based on features like square footage and number of bedrooms.\n",
    "Logistic Regression:\n",
    "\n",
    "Used for predicting the probability of a binary outcome (0 or 1).\n",
    "Applies a logistic function to the weighted sum of input features plus a bias term.\n",
    "Example: Predicting whether a student passes (1) or fails (0) an exam based on study hours.\n",
    "Scenario for Logistic Regression:\n",
    "Logistic regression is more appropriate when the dependent variable is binary or categorical, like predicting whether an \n",
    "email is spam (1) or not (0), whether a customer will purchase a product (1) or not (0), etc.\n",
    "\n",
    "Q2. Cost Function and Optimization in Logistic Regression:\n",
    "\n",
    "Measures the difference between predicted probabilities and actual labels.\n",
    "Optimization:\n",
    "\n",
    "Gradient Descent or other optimization algorithms are used to minimize the cost function.\n",
    "Update parameters \n",
    "Î¸ iteratively to find the minimum.\n",
    "Q3. Regularization in Logistic Regression:\n",
    "\n",
    "Concept:\n",
    "\n",
    "Regularization adds a penalty term to the cost function to prevent overfitting.\n",
    "Two types: L1 regularization (Lasso) and L2 regularization (Ridge).\n",
    "Preventing Overfitting:\n",
    "\n",
    "Regularization discourages overly complex models by penalizing large coefficients.\n",
    "Helps control model complexity and generalize better to new data.\n",
    "Q4. ROC Curve and Performance Evaluation:\n",
    "\n",
    "ROC Curve (Receiver Operating Characteristic):\n",
    "\n",
    "A graphical representation of the trade-off between sensitivity (true positive rate) and specificity (true negative rate).\n",
    "Plots the true positive rate against the false positive rate at various classification thresholds.\n",
    "Evaluation:\n",
    "\n",
    "The area under the ROC curve (AUC-ROC) quantifies the model's ability to distinguish between positive and \n",
    "negative instances.\n",
    "A higher AUC-ROC indicates better performance.\n",
    "Q5. Feature Selection Techniques in Logistic Regression:\n",
    "\n",
    "Forward Selection and Backward Elimination:\n",
    "Iteratively add or remove features based on statistical criteria.\n",
    "L1 Regularization (Lasso):\n",
    "Encourages sparsity by setting some coefficients to zero.\n",
    "Recursive Feature Elimination (RFE):\n",
    "Recursively removes the least significant features.\n",
    "Information Gain or Mutual Information:\n",
    "Measures the dependence between features and the target variable.\n",
    "Benefits:\n",
    "\n",
    "Reduces dimensionality.\n",
    "Improves model interpretability.\n",
    "Can prevent overfitting.\n",
    "Q6. Handling Imbalanced Datasets in Logistic Regression:\n",
    "\n",
    "Strategies:\n",
    "\n",
    "Resampling: Oversampling minority class or undersampling majority class.\n",
    "Synthetic Data Generation: Techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "Different Thresholds: Adjusting classification thresholds to balance precision and recall.\n",
    "Importance:\n",
    "\n",
    "Prevents the model from being biased towards the majority class.\n",
    "Improves sensitivity to the minority class.\n",
    "Q7. Common Issues and Challenges in Logistic Regression:\n",
    "\n",
    "Multicollinearity:\n",
    "\n",
    "Issue: High correlation among independent variables.\n",
    "Solution: Remove one of the correlated variables or use regularization techniques.\n",
    "Outliers:\n",
    "\n",
    "Issue: Outliers can influence the model.\n",
    "Solution: Detect and handle outliers using techniques like trimming or transforming.\n",
    "Non-Linearity:\n",
    "\n",
    "Issue: Logistic regression assumes a linear relationship.\n",
    "Solution: Consider polynomial terms or use more complex models if needed.\n",
    "Large Feature Space:\n",
    "\n",
    "Issue: High-dimensional datasets can lead to overfitting.\n",
    "Solution: Feature selection techniques or regularization to control model complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

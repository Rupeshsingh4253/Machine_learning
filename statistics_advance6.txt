**Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.**

**Assumptions of ANOVA:**
1. **Normality:** The residuals of each group should be approximately normally distributed.
2. **Homogeneity of Variances (Homoscedasticity):** The variances of the residuals should be equal across all groups.
3. **Independence:** Observations within and between groups should be independent.

**Examples of Violations:**
1. **Non-Normality:** If the residuals are not normally distributed, it may affect the accuracy of p-values and confidence intervals.
2. **Heteroscedasticity:** Unequal variances can lead to incorrect standard errors and, consequently, inaccurate hypothesis testing.
3. **Non-Independence:** If observations are not independent, it can lead to biased standard errors and incorrect statistical inferences.

**Q2. What are the three types of ANOVA, and in what situations would each be used?**

1. **One-Way ANOVA:** Used when comparing means across more than two independent groups. It tests if there are any statistically significant differences between the means of these groups.

2. **Two-Way ANOVA:** Used when there are two independent categorical variables, and we want to study the effect of each variable on the dependent variable. It examines both main effects and interaction effects.

3. **Repeated Measures ANOVA:** Used when the same subjects are used for each treatment (e.g., repeated measurements over time). It analyzes whether there are any statistically significant differences between the means of related groups.

**Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?**

The partitioning of variance in ANOVA involves dividing the total variance observed in the data into different components:
- **Total Sum of Squares (SST):** Variability of the dependent variable.
- **Explained Sum of Squares (SSE):** Variability explained by the model (group means).
- **Residual Sum of Squares (SSR):** Unexplained variability or error.

Understanding this partitioning is crucial because it helps assess how much of the total variability in the data is due to group differences (SSE) and how much is due to random variability or error (SSR).

**Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?**

To calculate SST, SSE, and SSR in Python, you can use the following:

```python
import numpy as np
import scipy.stats as stats

# Example data for three groups
group1 = np.array([1, 2, 3, 4, 5])
group2 = np.array([6, 7, 8, 9, 10])
group3 = np.array([11, 12, 13, 14, 15])

# Concatenate the data
all_data = np.concatenate([group1, group2, group3])

# Calculate means
grand_mean = np.mean(all_data)
mean_group1 = np.mean(group1)
mean_group2 = np.mean(group2)
mean_group3 = np.mean(group3)

# Calculate SST, SSE, and SSR
sst = np.sum((all_data - grand_mean)**2)
sse = np.sum((group1 - mean_group1)**2) + np.sum((group2 - mean_group2)**2) + np.sum((group3 - mean_group3)**2)
ssr = sst - sse

print(f"Total Sum of Squares (SST): {sst}")
print(f"Explained Sum of Squares (SSE): {sse}")
print(f"Residual Sum of Squares (SSR): {ssr}")
```

**Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?**

To calculate main effects and interaction effects in a two-way ANOVA in Python, you can use the `statsmodels` library:

```python
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Example data
# Assume 'factor1' and 'factor2' are the two categorical variables, and 'response' is the dependent variable
data = pd.DataFrame({'factor1': ['A', 'A', 'B', 'B', 'C', 'C'],
                     'factor2': ['X', 'Y', 'X', 'Y', 'X', 'Y'],
                     'response': [10, 12, 15, 14, 18, 20]})

# Fit the two-way ANOVA model
model = ols('response ~ factor1 * factor2', data=data).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

# Extract main effects and interaction effects
main_effect_factor1 = anova_table.loc['factor1', 'sum_sq'] / anova_table.loc['factor1', 'df']
main_effect_factor2 = anova_table.loc['factor2', 'sum_sq'] / anova_table.loc['factor2', 'df']
interaction_effect = anova_table.loc['factor1:factor2', 'sum_sq'] / anova_table.loc['factor1:factor2', 'df']

print(f"Main Effect of Factor 1: {main_effect_factor1}")
print(f"Main Effect of Factor 2: {main_effect_factor2}")
print(f"Interaction Effect: {interaction_effect}")
```

**Note:** Make sure to have the necessary libraries imported, including `numpy`, `pandas`, and `scipy.stats`.

Continued...

**Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?**

In a one-way ANOVA, the F-statistic tests whether there are significant differences between the means of three or more groups. The obtained F-statistic of 5.23 indicates that there is some evidence of differences among the group means. The associated p-value of 0.02 is below the conventional significance level of 0.05.

Interpretation:
- **Null Hypothesis (H0):** There is no significant difference between the group means.
- **Alternative Hypothesis (H1):** At least one group mean is different from the others.

Since the p-value is less than 0.05, we reject the null hypothesis. Therefore, we have evidence to suggest that there are significant differences between the means of the groups.

**Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?**

Handling missing data in repeated measures ANOVA involves choosing an appropriate method, such as:
1. **Complete Case Analysis (CCA):** Exclude participants with missing data.
2. **Mean Imputation:** Replace missing values with the mean of the observed values.
3. **Interpolation or Last Observation Carried Forward (LOCF):** Use adjacent data points to estimate missing values.

Consequences:
- **Complete Case Analysis (CCA):** May lead to biased results if missing data is not completely random.
- **Mean Imputation:** Can artificially reduce variability and bias results towards the mean.
- **Interpolation/LOCF:** Assumes a linear relationship and may not capture the true pattern of change.

The choice depends on the nature of the missing data and its potential impact on study validity.

**Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.**

Common post-hoc tests after ANOVA include:
1. **Tukey's Honestly Significant Difference (HSD):** Used when there are more than two groups. Controls the familywise error rate.
2. **Bonferroni Correction:** Adjusts significance levels for multiple comparisons. Used when testing specific pairs of groups.
3. **Sidak Correction:** Similar to Bonferroni but less conservative.

Post-hoc tests are necessary when ANOVA indicates significant differences among groups, but it doesn't identify which specific groups differ.

Example: Suppose a one-way ANOVA reveals a significant difference in mean test scores among four teaching methods. A post-hoc test like Tukey's HSD could be used to determine which pairs of teaching methods have significantly different mean test scores.

**Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results.**

```python
import scipy.stats as stats

# Example data
diet_A = [2, 3, 4, 2, 5, 6, 7, 3, 4, 5, 6, 8, 3, 4, 5, 4, 3, 5, 6, 7, 8, 9, 6, 5, 4, 3, 5, 7, 6, 8, 9, 5, 4, 3, 6, 7, 8, 9, 5, 4, 3, 5, 6, 7, 8, 9, 5, 4, 3]
diet_B = [1, 2, 3, 1, 4, 5, 6, 2, 3, 4, 5, 7, 2, 3, 4, 3, 2, 4, 5, 6, 7, 8, 5, 4, 3, 2, 4, 6, 5, 7, 8, 4, 3, 2, 5, 6, 7, 8, 4, 3, 2, 4, 5, 6, 7, 8, 4, 3, 2]
diet_C = [5, 6, 7, 5, 8, 9, 10, 6, 7, 8, 9, 11, 6, 7, 8, 7, 6, 8, 9, 10, 11, 12, 9, 8, 7, 6, 8, 10, 9, 11, 12, 8, 7, 6, 9, 10, 11, 12, 8, 7, 6, 8, 9, 10, 11, 12, 8

**Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results.**

```python
import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Example data
data = pd.DataFrame({
    'Program': ['A', 'B', 'C'] * 30,
    'Experience': ['Novice', 'Experienced'] * 45,
    'Time': [15, 18, 20, 17, 22, 25, 19, 21, 23] * 10  # Sample data, adjust as needed
})

# Fit the two-way ANOVA model
model = ols('Time ~ Program * Experience', data=data).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

# Extract F-statistics and p-values
f_program = anova_table.loc['Program', 'F']
p_program = anova_table.loc['Program', 'PR(>F)']
f_experience = anova_table.loc['Experience', 'F']
p_experience = anova_table.loc['Experience', 'PR(>F)']
f_interaction = anova_table.loc['Program:Experience', 'F']
p_interaction = anova_table.loc['Program:Experience', 'PR(>F)']

# Interpretation
print(f"Main Effect of Program: F = {f_program}, p = {p_program}")
print(f"Main Effect of Experience: F = {f_experience}, p = {p_experience}")
print(f"Interaction Effect: F = {f_interaction}, p = {p_interaction}")
```

Interpretation:
- A significant p-value for the main effect of Program would suggest that there are differences in average task completion time between the software programs.
- A significant p-value for the main effect of Experience would suggest that there are differences in average task completion time between novice and experienced employees.
- A significant p-value for the interaction effect would suggest that the effect of the software program on task completion time depends on the experience level.

**Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other.**

```python
import scipy.stats as stats

# Example data
control_group = [75, 78, 80, 72, 77, 82, 79, 85, 88, 73, 76, 81, 80, 78, 74, 79, 83, 75, 77, 80]
experimental_group = [85, 88, 90, 82, 87, 92, 89, 95, 98, 83, 86, 91, 90, 88, 84, 89, 93, 85, 87, 90]

# Two-sample t-test
t_stat, p_value = stats.ttest_ind(control_group, experimental_group)

# Interpretation
print(f"Two-sample t-test: t = {t_stat}, p = {p_value}")

# Post-hoc test (e.g., Tukey's HSD) if needed
# Post-hoc tests are typically used in ANOVA, not in a two-sample t-test scenario.
# However, if you need to compare multiple groups, consider using ANOVA with post-hoc tests.
```

Interpretation:
- If the p-value is below the significance level (e.g., 0.05), you reject the null hypothesis, suggesting a significant difference in test scores between the control and experimental groups.

**Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other.**

```python
import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Example data
data = pd.DataFrame({
    'Day': list(range(1, 31)) * 3,
    'Store': ['A'] * 30 + ['B'] * 30 + ['C'] * 30,
    'Sales': [50, 55, 48, 52, 54, 58, 53, 50, 55, 49, 51, 56, 52, 54, 59, 55, 50, 53, 47, 52, 55, 58, 54, 56, 50, 52, 55, 49, 51, 53] * 3
})

# Fit the repeated measures ANOVA model
model = ols('Sales ~ Store + C(Day)', data=data).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

# Extract F-statistics and p-values
f_store = anova_table.loc['Store', 'F']
p_store = anova_table.loc['Store', 'PR(>F)']

# Interpretation
print(f"Repeated Measures ANOVA - Store: F = {f_store}, p = {p_store}")

# Post-hoc test (e.g., Tukey's HSD) if needed
# Post-hoc tests are typically used in ANOVA, not in a repeated measures ANOVA scenario.
# However, if you need to compare multiple groups, consider using ANOVA with post-hoc tests.
```

Interpretation:
- If the p-value for the main effect of Store is below the significance level, you can conclude that there are significant differences in average daily sales between the three stores.


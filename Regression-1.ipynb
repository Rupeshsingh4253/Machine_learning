{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d926794",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Simple Linear Regression vs. Multiple Linear Regression:\n",
    "\n",
    "Simple Linear Regression: In simple linear regression, there is a single independent variable predicting a dependent variable.\n",
    "The relationship is modeled as a straight line.\n",
    "Multiple Linear Regression: In multiple linear regression, there are multiple independent variables predicting a dependent\n",
    "variable. The relationship is modeled as a hyperplane. \n",
    "\n",
    "Q2. Assumptions of Linear Regression:\n",
    "\n",
    "Linearity, Independence, Homoscedasticity, Normality of residuals.\n",
    "Checking assumptions involves using diagnostic plots, residuals analysis, and statistical tests.\n",
    "Q3. Interpretation of Slope and Intercept:\n",
    "Q3. Interpretation of Slope and Intercept:\n",
    "\n",
    "Slope (\n",
    "b): It represents the change in the dependent variable for a one-unit change in the independent variable. If \n",
    "b is positive, it indicates a positive relationship, and if negative, a negative relationship.\n",
    "Intercept \n",
    "a): It represents the predicted value of the dependent variable when all independent variables are zero.\n",
    "Example: In a salary prediction model, the slope for years of experience \n",
    "b) might be 2000, indicating that each additional year of experience is associated with a $2000 increase in salary.\n",
    "\n",
    "Q4. Gradient Descent:\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models.\n",
    "It involves iteratively adjusting model parameters in the direction opposite to the gradient of the cost function to find the minimum.\n",
    "It is crucial in training models like linear regression, neural networks, and others.\n",
    "Q5. Multiple Linear Regression Model:\n",
    "\n",
    "Q6. Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Multicollinearity occurs when independent variables are highly correlated.\n",
    "Detection: Variance Inflation Factor (VIF) or correlation matrices.\n",
    "Addressing: Remove/reduce correlated variables or use regularization techniques.\n",
    "Q7. Polynomial Regression Model:\n",
    "\n",
    "Polynomial regression models the relationship between the independent variable and the dependent variable as an nth-degree\n",
    "polynomial.\n",
    "It allows capturing more complex relationships.\n",
    "Q8. Advantages and Disadvantages of Polynomial Regression:\n",
    "\n",
    "Advantages: Captures non-linear relationships, more flexible.\n",
    "Disadvantages: Prone to overfitting, computationally expensive.\n",
    "Use Cases: Suitable when the relationship between variables is not linear and there is a need for a more flexible model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Polynomial functions and kernel functions in machine learning algorithms are related through the use of polynomial \n",
    "    kernels in support vector machines (SVMs). \n",
    "    Polynomial kernels are a type of kernel function used in SVMs to transform the input features into a higher-dimensional\n",
    "    space, allowing the SVM to find a hyperplane that separates the data in a non-linear way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bf52a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (replace this with your dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling in this example)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "svc_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels for the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance using accuracy as a metric\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: In Support Vector Regression (SVR), increasing the value of epsilon.\n",
    "    allows for a wider margin of error within which data points are not considered as violations. As epsilon increases, \n",
    "    more data points may fall within the margin, and the number of support vectors may decrease. Conversely, decreasing \n",
    "    epsilon may lead to a narrower margin and an increase in the number of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: The choice of kernel function, C parameter, epsilon parameter, and gamma parameter in Support Vector Regression (SVR) can\n",
    "    significantly impact its performance:\n",
    "\n",
    "Kernel Function:\n",
    "\n",
    "Polynomial and radial basis function (RBF) kernels are commonly used. The choice depends on the nature of the data. Polynomial\n",
    "kernels are suitable for non-linear relationships, while RBF kernels are more flexible.\n",
    "C Parameter:\n",
    "\n",
    "The C parameter controls the trade-off between fitting the training data and having a smooth decision boundary. A smaller C\n",
    "allows for a smoother decision boundary, potentially sacrificing fitting the training data closely, while a larger C may lead\n",
    "to overfitting.\n",
    "Epsilon Parameter: determines the width of the epsilon-insensitive tube. A larger epsilon allows more training points to be within the tube \n",
    "without penalty, potentially leading to a wider margin and fewer support vectors.\n",
    "Gamma Parameter:defines how far the influence of a single training example reaches. A small gamma creates a larger similarity radius,\n",
    "and a large gamma makes the model focus on closer data points. It strongly influences the shape of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d51448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Assuming X_train_scaled, y_train, X_test_scaled, y_test are already defined\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'poly', 'rbf'], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto']}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(svc_classifier, param_grid, cv=3, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc_classifier = grid_search.best_estimator_\n",
    "best_svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(best_svc_classifier, 'tuned_svc_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fa8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
